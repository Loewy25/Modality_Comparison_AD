{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/l.peiwang/.local/lib/python3.9/site-packages/nilearn/input_data/__init__.py:27: FutureWarning: The import path 'nilearn.input_data' is deprecated in version 0.9. Importing from 'nilearn.input_data' will be possible at least until release 0.13.0. Please import from 'nilearn.maskers' instead.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from nilearn.input_data import NiftiMasker\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "def augment_data(image):\n",
    "    augmented_image = image.copy()\n",
    "    for axis in range(3):\n",
    "        if np.random.rand() > 0.5:\n",
    "            augmented_image = np.flip(augmented_image, axis=axis)\n",
    "    return augmented_image\n",
    "\n",
    "def loading_mask(task,modality):\n",
    "    #Loading and generating data\n",
    "    images_pet,images_mri,labels=generate_data_path()\n",
    "    if modality == 'PET':\n",
    "        data_train,train_label=generate(images_pet,labels,task)\n",
    "    if modality == 'MRI':\n",
    "        data_train,train_label=generate(images_mri,labels,task)\n",
    "    masker = NiftiMasker(mask_img='/home/l.peiwang/MR-PET-Classfication/mask_gm_p4_new4.nii')\n",
    "    train_data=[]\n",
    "    for i in range(len(data_train)):\n",
    "        # Apply masker and inverse transform\n",
    "        masked_data = masker.fit_transform(data_train[i])\n",
    "        masked_image = masker.inverse_transform(masked_data)\n",
    "\n",
    "        # Resize the image\n",
    "        resized_image = resize(masked_image)\n",
    "\n",
    "        # Convert the resized NIfTI image to a numpy array\n",
    "        resized_data = resized_image.get_fdata()\n",
    "        train_data.append(resized_data)\n",
    "      \n",
    "    train_label=binarylabel(train_label,task)\n",
    "    \n",
    "    return train_data,train_label,masker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv3D, Input, LeakyReLU, Add, GlobalAveragePooling3D, Dense, Dropout, SpatialDropout3D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import nibabel as nib\n",
    "\n",
    "import numpy as np\n",
    "from nilearn.image import resample_img, new_img_like, reorder_img\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "def resample_to_spacing(data, original_spacing, new_spacing, interpolation='linear'):\n",
    "    # Assuming the last dimension is the channel and should not be resampled\n",
    "    zoom_factors = [o / n for o, n in zip(original_spacing, new_spacing)] + [1]\n",
    "    return zoom(data, zoom_factors, order=1 if interpolation == 'linear' else 0)\n",
    "\n",
    "\n",
    "def calculate_origin_offset(new_spacing, original_spacing):\n",
    "    return [(o - n) / 2 for o, n in zip(original_spacing, new_spacing)]\n",
    "\n",
    "\n",
    "def pad_image_to_shape(image, target_shape=(128, 128, 128,1)):\n",
    "    # Calculate the padding required in each dimension\n",
    "    padding = [(0, max(target_shape[dim] - image.shape[dim], 0)) for dim in range(3)]\n",
    "    \n",
    "    # Apply zero-padding to the data\n",
    "    new_data = np.pad(image.get_fdata(), padding, mode='constant', constant_values=0)\n",
    "    \n",
    "    # Adjust the affine to account for the new shape\n",
    "    new_affine = np.copy(image.affine)\n",
    "    \n",
    "    # Create and return a new NIfTI-like image with the padded data\n",
    "    return new_img_like(image, new_data, affine=new_affine)\n",
    "\n",
    "\n",
    "\n",
    "def resize(image, new_shape=(128, 128, 128), interpolation=\"linear\"):\n",
    "    # Reorder the image and resample it with the desired interpolation\n",
    "    image = reorder_img(image, resample=interpolation)\n",
    "    \n",
    "    # Calculate the zoom levels needed for the new shape\n",
    "    zoom_level = np.divide(new_shape, image.shape[:3])\n",
    "    \n",
    "    # Calculate the new spacing for the image\n",
    "    new_spacing = np.divide(image.header.get_zooms()[:3], zoom_level)\n",
    "    \n",
    "    # Resample the image data to the new spacing\n",
    "    new_data = resample_to_spacing(image.get_fdata(), image.header.get_zooms()[:3], new_spacing, \n",
    "                                   interpolation=interpolation)\n",
    "    # Copy and adjust the affine transformation matrix for the new spacing\n",
    "    new_affine = np.copy(image.affine)\n",
    "    np.fill_diagonal(new_affine, new_spacing.tolist() + [1])\n",
    "    new_affine[:3, 3] += calculate_origin_offset(new_spacing, image.header.get_zooms()[:3])\n",
    "    \n",
    "    # Create and return a new NIfTI-like image\n",
    "    return new_img_like(image, new_data, affine=new_affine)\n",
    "\n",
    "\n",
    "\n",
    "def convolution_block(x, filters, kernel_size=(3,3,3), strides=(1,1,1)):\n",
    "    x = Conv3D(filters, kernel_size, strides=strides, padding='same', kernel_regularizer=l2(1e-5))(x)\n",
    "    x = tfa.layers.InstanceNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    return x\n",
    "\n",
    "def context_module(x, filters):\n",
    "    # First convolution block\n",
    "    x = convolution_block(x, filters)\n",
    "    # Dropout layer\n",
    "    x = SpatialDropout3D(0.3)(x) \n",
    "    # Second convolution block\n",
    "    x = convolution_block(x, filters)\n",
    "    return x\n",
    "\n",
    "def create_cnn_model():\n",
    "    input_img = Input(shape=(128, 128, 128, 1))\n",
    "    x = convolution_block(input_img, 16, strides=(1,1,1))\n",
    "    conv1_out = x\n",
    "\n",
    "    # Context 1\n",
    "    x = context_module(x, 16)\n",
    "    x = Add()([x, conv1_out])\n",
    "    x = convolution_block(x, 32, strides=(2,2,2))\n",
    "    conv2_out = x\n",
    "\n",
    "    # Context 2\n",
    "    x = context_module(x, 32)\n",
    "    x = Add()([x, conv2_out])\n",
    "    x = convolution_block(x, 64, strides=(2,2,2))\n",
    "    conv3_out = x\n",
    "\n",
    "    # Context 3\n",
    "    x = context_module(x, 64)\n",
    "    x = Add()([x, conv3_out])\n",
    "    x = convolution_block(x, 128, strides=(2,2,2))\n",
    "    conv4_out = x\n",
    "\n",
    "    # Context 4\n",
    "    x = context_module(x, 128)\n",
    "    x = Add()([x, conv4_out])\n",
    "    x = convolution_block(x, 256, strides=(2,2,2))\n",
    "    \n",
    "    # Context 5\n",
    "    x = context_module(x, 256)\n",
    "\n",
    "    # Global Average Pooling\n",
    "    x = GlobalAveragePooling3D()(x)\n",
    "\n",
    "    # Dropout layer as described in the paper\n",
    "    x = SpatialDropout3D(0.3)(x)   # The paper mentioned a dropout layer after GAP\n",
    "\n",
    "    # Dense layer with 7 output nodes as described in the paper\n",
    "    output = Dense(2, activation='softmax')(x) \n",
    "\n",
    "    model = Model(inputs=input_img, outputs=output)\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dicom2nifti\n",
    "import glob\n",
    "import math\n",
    "import nibabel as nib\n",
    "import nilearn as nil\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import scipy.ndimage as ndi\n",
    "import statsmodels.stats.contingency_tables as ct\n",
    "import time\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from nilearn import image, plotting\n",
    "from nilearn.input_data import NiftiMasker\n",
    "from nilearn.masking import (apply_mask, compute_brain_mask,\n",
    "                             compute_multi_brain_mask, intersect_masks, unmask)\n",
    "from nilearn.plotting import plot_roi, plot_stat_map, show\n",
    "from numpy import mean, std\n",
    "from numpy.linalg import inv\n",
    "from scipy.stats import chi2_contingency, norm\n",
    "from sklearn import metrics, svm\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import (accuracy_score, confusion_matrix,\n",
    "                             precision_recall_curve, precision_recall_fscore_support,\n",
    "                             roc_auc_score, roc_curve)\n",
    "from sklearn.model_selection import (GridSearchCV, KFold, StratifiedKFold,\n",
    "                                     cross_val_predict, cross_val_score,\n",
    "                                     train_test_split)\n",
    "from sklearn.preprocessing import Binarizer, label_binarize\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate(images,labels,task):\n",
    "    imagesData=[]\n",
    "    labelsData=[]\n",
    "    cn=0\n",
    "    pcn=0\n",
    "    dementia=0\n",
    "    mci=0\n",
    "    for i in range(len(images)):\n",
    "      if labels[i]=='CN':\n",
    "        cn+=1\n",
    "      if labels[i]=='MCI':\n",
    "        mci+=1\n",
    "      if labels[i]=='Dementia':\n",
    "        dementia+=1\n",
    "      if labels[i]=='PCN':\n",
    "        pcn+=1\n",
    "    print(\"Number of CN subjects:\")\n",
    "    print(cn)\n",
    "    print(\"Number of PCN subjects:\")\n",
    "    print(pcn)\n",
    "    print(\"Number of MCI subjects:\")\n",
    "    print(mci)\n",
    "    print(\"Number of Dementia subjects:\")\n",
    "    print(dementia)\n",
    "    if task == \"cd\":\n",
    "        for i in range(len(images)):\n",
    "            if labels[i] == \"CN\":\n",
    "                imagesData.append(images[i])\n",
    "                labelsData.append(labels[i])\n",
    "            if labels[i] == \"Dementia\":\n",
    "                imagesData.append(images[i])\n",
    "                labelsData.append(labels[i])\n",
    "    if task == \"cm\":\n",
    "        for i in range(len(images)):\n",
    "            if labels[i] == \"CN\":\n",
    "                imagesData.append(images[i])\n",
    "                labelsData.append(labels[i])\n",
    "            if labels[i] == \"MCI\":\n",
    "                imagesData.append(images[i])\n",
    "                labelsData.append(labels[i])\n",
    "    if task == \"dm\":\n",
    "        for i in range(len(images)):\n",
    "            if labels[i] == \"Dementia\":\n",
    "                imagesData.append(images[i])\n",
    "                labelsData.append(labels[i])\n",
    "            if labels[i] == \"MCI\":\n",
    "                imagesData.append(images[i])\n",
    "                labelsData.append(labels[i])\n",
    "    if task == \"pc\":\n",
    "        for i in range(len(images)):\n",
    "            if labels[i] == \"PCN\":\n",
    "                imagesData.append(images[i])\n",
    "                labelsData.append(labels[i])\n",
    "            if labels[i] == \"CN\":\n",
    "                imagesData.append(images[i])\n",
    "                labelsData.append(labels[i])   \n",
    "    if task == 'cdm':\n",
    "        for i in range(len(images)):\n",
    "            if labels[i] == \"CN\":\n",
    "                imagesData.append(images[i])\n",
    "                labelsData.append(labels[i])\n",
    "            if labels[i] == \"Dementia\" or labels[i] == 'MCI':\n",
    "                imagesData.append(images[i])\n",
    "                labelsData.append(labels[i])\n",
    "    print(\"lenth of dataset: \")\n",
    "    print(len(labelsData))\n",
    "      \n",
    "        \n",
    "    return imagesData,labelsData\n",
    "\n",
    "\n",
    "def generate_data_path():\n",
    "    files=['/scratch/jjlee/Singularity/ADNI/bids/derivatives/table_preclinical_cross-sectional.csv','/scratch/jjlee/Singularity/ADNI/bids/derivatives/table_cn_cross-sectional.csv','/scratch/jjlee/Singularity/ADNI/bids/derivatives/table_cdr_0p5_apos_cross-sectional.csv','/scratch/jjlee/Singularity/ADNI/bids/derivatives/table_cdr_gt_0p5_apos_cross-sectional.csv']\n",
    "    class_labels=['PCN','CN','MCI','Dementia']\n",
    "    pet_paths = []\n",
    "    mri_paths = []\n",
    "    class_labels_out = []\n",
    "\n",
    "    for file, class_label in zip(files, class_labels):\n",
    "        df = pd.read_csv(file)\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            # Extract sub-xxxx and ses-xxxx from original paths\n",
    "            sub_ses_info = \"/\".join(row['FdgFilename'].split(\"/\")[8:10])\n",
    "\n",
    "            # Generate new directory\n",
    "            new_directory = os.path.join('/scratch/l.peiwang/derivatives_new', sub_ses_info, 'pet')\n",
    "\n",
    "            # Get all files that match the pattern but then exclude ones that contain 'icv'\n",
    "            pet_files = [f for f in glob.glob(new_directory + '/*FDG*') if 'icv' not in f]\n",
    "            mri_files = glob.glob(new_directory + '/*detJ*icv*')\n",
    "            if pet_files and mri_files:  # If both lists are not empty\n",
    "                pet_paths.append(pet_files[0])  # Append the first PET file found\n",
    "                mri_paths.append(mri_files[0])  # Append the first MRI file found\n",
    "                class_labels_out.append(class_label)  # Associate class label with the path\n",
    "\n",
    "    return pet_paths, mri_paths, class_labels_out\n",
    "\n",
    "\n",
    "def binarylabel(train_label,mode):\n",
    "    if mode==\"cd\" or mode==\"cdm\":\n",
    "        for i in range(len(train_label)):\n",
    "            if train_label[i]==\"CN\":\n",
    "                train_label[i]=0\n",
    "            else:\n",
    "                train_label[i]=1\n",
    "    if mode==\"cm\":\n",
    "        for i in range(len(train_label)):\n",
    "            if train_label[i]==\"CN\":\n",
    "                train_label[i]=0\n",
    "            else:\n",
    "                train_label[i]=1\n",
    "    if mode==\"dm\":\n",
    "        for i in range(len(train_label)):\n",
    "            if train_label[i]==\"Dementia\":\n",
    "                train_label[i]=1\n",
    "            else:\n",
    "                train_label[i]=0\n",
    "    if mode==\"pc\":\n",
    "        for i in range(len(train_label)):\n",
    "            if train_label[i]==\"CN\":\n",
    "                train_label[i]=0\n",
    "            else:\n",
    "                train_label[i]=1\n",
    "    if mode==\"pm\":\n",
    "        for i in range(len(train_label)):\n",
    "            if train_label[i]==\"EMCI\":\n",
    "                train_label[i]=1\n",
    "            else:\n",
    "                train_label[i]=0\n",
    "    return train_label\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CN subjects:\n",
      "263\n",
      "Number of PCN subjects:\n",
      "140\n",
      "Number of MCI subjects:\n",
      "458\n",
      "Number of Dementia subjects:\n",
      "151\n",
      "lenth of dataset: \n",
      "414\n"
     ]
    }
   ],
   "source": [
    "def augment_data(image):\n",
    "    augmented_image = image.copy()\n",
    "    for axis in range(3):\n",
    "        if np.random.rand() > 0.5:\n",
    "            augmented_image = np.flip(augmented_image, axis=axis)\n",
    "    return augmented_image\n",
    "\n",
    "def loading_mask(task,modality):\n",
    "    #Loading and generating data\n",
    "    images_pet,images_mri,labels=generate_data_path()\n",
    "    if modality == 'PET':\n",
    "        data_train,train_label=generate(images_pet,labels,task)\n",
    "    if modality == 'MRI':\n",
    "        data_train,train_label=generate(images_mri,labels,task)\n",
    "    masker = NiftiMasker(mask_img='/home/l.peiwang/MR-PET-Classfication/mask_gm_p4_new4.nii')\n",
    "    train_data=[]\n",
    "    for i in range(len(data_train)):\n",
    "        # Apply masker and inverse transform\n",
    "        masked_data = masker.fit_transform(data_train[i])\n",
    "        masked_image = masker.inverse_transform(masked_data)\n",
    "\n",
    "        # Resize the image\n",
    "        resized_image = resize(masked_image)\n",
    "\n",
    "        # Convert the resized NIfTI image to a numpy array\n",
    "        resized_data = resized_image.get_fdata()\n",
    "        train_data.append(resized_data)\n",
    "      \n",
    "    train_label=binarylabel(train_label,task)\n",
    "    \n",
    "    return train_data,train_label,masker\n",
    "\n",
    "\n",
    "task = 'cd'\n",
    "modality = 'PET'\n",
    "train_data, train_label, masker = loading_mask(task, modality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(414, 128, 128, 128, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
