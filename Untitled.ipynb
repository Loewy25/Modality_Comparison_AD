{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "A numpy version of at least 1.16 is required to use nilearn. 0.0.0 was found. Please upgrade numpy",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnilearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minput_data\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NiftiMasker\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StratifiedKFold\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maugment_data\u001b[39m(image):\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/nilearn/__init__.py:68\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mversion_info\u001b[38;5;241m.\u001b[39mmajor \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mversion_info\u001b[38;5;241m.\u001b[39mminor \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m6\u001b[39m:\n\u001b[1;32m     65\u001b[0m         _py36_deprecation_warning()\n\u001b[0;32m---> 68\u001b[0m \u001b[43m_check_module_dependencies\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m _python_deprecation_warnings()\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Monkey-patch gzip to have faster reads on large gzip files\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/nilearn/version.py:180\u001b[0m, in \u001b[0;36m_check_module_dependencies\u001b[0;34m(is_nilearn_installing)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (module_name, module_metadata) \u001b[38;5;129;01min\u001b[39;00m REQUIRED_MODULE_METADATA:\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_nilearn_installing \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    177\u001b[0m             \u001b[38;5;129;01mnot\u001b[39;00m module_metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrequired_at_installation\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;66;03m# Skip check only when installing and it's a module that\u001b[39;00m\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;66;03m# will be auto-installed.\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m         \u001b[43m_import_module_with_version_check\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m            \u001b[49m\u001b[43mminimum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule_metadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmin_version\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m            \u001b[49m\u001b[43minstall_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule_metadata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minstall_info\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/nilearn/version.py:100\u001b[0m, in \u001b[0;36m_import_module_with_version_check\u001b[0;34m(module_name, minimum_version, install_info)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version_too_old:\n\u001b[1;32m     92\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     93\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA \u001b[39m\u001b[38;5;132;01m{module_name}\u001b[39;00m\u001b[38;5;124m version of at least \u001b[39m\u001b[38;5;132;01m{minimum_version}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis required to use nilearn. \u001b[39m\u001b[38;5;132;01m{module_version}\u001b[39;00m\u001b[38;5;124m was found. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     97\u001b[0m             minimum_version\u001b[38;5;241m=\u001b[39mminimum_version,\n\u001b[1;32m     98\u001b[0m             module_version\u001b[38;5;241m=\u001b[39mmodule_version)\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(message)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\n",
      "\u001b[0;31mImportError\u001b[0m: A numpy version of at least 1.16 is required to use nilearn. 0.0.0 was found. Please upgrade numpy"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from nilearn.input_data import NiftiMasker\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "def augment_data(image):\n",
    "    augmented_image = image.copy()\n",
    "    for axis in range(3):\n",
    "        if np.random.rand() > 0.5:\n",
    "            augmented_image = np.flip(augmented_image, axis=axis)\n",
    "    return augmented_image\n",
    "\n",
    "def loading_mask(task,modality):\n",
    "    #Loading and generating data\n",
    "    images_pet,images_mri,labels=generate_data_path()\n",
    "    if modality == 'PET':\n",
    "        data_train,train_label=generate(images_pet,labels,task)\n",
    "    if modality == 'MRI':\n",
    "        data_train,train_label=generate(images_mri,labels,task)\n",
    "    masker = NiftiMasker(mask_img='/home/l.peiwang/MR-PET-Classfication/mask_gm_p4_new4.nii')\n",
    "    train_data=[]\n",
    "    for i in range(len(data_train)):\n",
    "        # Apply masker and inverse transform\n",
    "        masked_data = masker.fit_transform(data_train[i])\n",
    "        masked_image = masker.inverse_transform(masked_data)\n",
    "\n",
    "        # Resize the image\n",
    "        resized_image = resize(masked_image)\n",
    "\n",
    "        # Convert the resized NIfTI image to a numpy array\n",
    "        resized_data = resized_image.get_fdata()\n",
    "        train_data.append(resized_data)\n",
    "      \n",
    "    train_label=binarylabel(train_label,task)\n",
    "    \n",
    "    return train_data,train_label,masker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv3D, Input, LeakyReLU, Add, GlobalAveragePooling3D, Dense, Dropout, SpatialDropout3D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import nibabel as nib\n",
    "\n",
    "import numpy as np\n",
    "from nilearn.image import resample_img, new_img_like, reorder_img\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "def resample_to_spacing(data, original_spacing, new_spacing, interpolation='linear'):\n",
    "    # Assuming the last dimension is the channel and should not be resampled\n",
    "    zoom_factors = [o / n for o, n in zip(original_spacing, new_spacing)] + [1]\n",
    "    return zoom(data, zoom_factors, order=1 if interpolation == 'linear' else 0)\n",
    "\n",
    "\n",
    "def calculate_origin_offset(new_spacing, original_spacing):\n",
    "    return [(o - n) / 2 for o, n in zip(original_spacing, new_spacing)]\n",
    "\n",
    "\n",
    "def pad_image_to_shape(image, target_shape=(128, 128, 128,1)):\n",
    "    # Calculate the padding required in each dimension\n",
    "    padding = [(0, max(target_shape[dim] - image.shape[dim], 0)) for dim in range(3)]\n",
    "    \n",
    "    # Apply zero-padding to the data\n",
    "    new_data = np.pad(image.get_fdata(), padding, mode='constant', constant_values=0)\n",
    "    \n",
    "    # Adjust the affine to account for the new shape\n",
    "    new_affine = np.copy(image.affine)\n",
    "    \n",
    "    # Create and return a new NIfTI-like image with the padded data\n",
    "    return new_img_like(image, new_data, affine=new_affine)\n",
    "\n",
    "\n",
    "\n",
    "def resize(image, new_shape=(128, 128, 128), interpolation=\"linear\"):\n",
    "    # Reorder the image and resample it with the desired interpolation\n",
    "    image = reorder_img(image, resample=interpolation)\n",
    "    \n",
    "    # Calculate the zoom levels needed for the new shape\n",
    "    zoom_level = np.divide(new_shape, image.shape[:3])\n",
    "    \n",
    "    # Calculate the new spacing for the image\n",
    "    new_spacing = np.divide(image.header.get_zooms()[:3], zoom_level)\n",
    "    \n",
    "    # Resample the image data to the new spacing\n",
    "    new_data = resample_to_spacing(image.get_fdata(), image.header.get_zooms()[:3], new_spacing, \n",
    "                                   interpolation=interpolation)\n",
    "    # Copy and adjust the affine transformation matrix for the new spacing\n",
    "    new_affine = np.copy(image.affine)\n",
    "    np.fill_diagonal(new_affine, new_spacing.tolist() + [1])\n",
    "    new_affine[:3, 3] += calculate_origin_offset(new_spacing, image.header.get_zooms()[:3])\n",
    "    \n",
    "    # Create and return a new NIfTI-like image\n",
    "    return new_img_like(image, new_data, affine=new_affine)\n",
    "\n",
    "\n",
    "\n",
    "def convolution_block(x, filters, kernel_size=(3,3,3), strides=(1,1,1)):\n",
    "    x = Conv3D(filters, kernel_size, strides=strides, padding='same', kernel_regularizer=l2(1e-5))(x)\n",
    "    x = tfa.layers.InstanceNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    return x\n",
    "\n",
    "def context_module(x, filters):\n",
    "    # First convolution block\n",
    "    x = convolution_block(x, filters)\n",
    "    # Dropout layer\n",
    "    x = SpatialDropout3D(0.3)(x) \n",
    "    # Second convolution block\n",
    "    x = convolution_block(x, filters)\n",
    "    return x\n",
    "\n",
    "def create_cnn_model():\n",
    "    input_img = Input(shape=(128, 128, 128, 1))\n",
    "    x = convolution_block(input_img, 16, strides=(1,1,1))\n",
    "    conv1_out = x\n",
    "\n",
    "    # Context 1\n",
    "    x = context_module(x, 16)\n",
    "    x = Add()([x, conv1_out])\n",
    "    x = convolution_block(x, 32, strides=(2,2,2))\n",
    "    conv2_out = x\n",
    "\n",
    "    # Context 2\n",
    "    x = context_module(x, 32)\n",
    "    x = Add()([x, conv2_out])\n",
    "    x = convolution_block(x, 64, strides=(2,2,2))\n",
    "    conv3_out = x\n",
    "\n",
    "    # Context 3\n",
    "    x = context_module(x, 64)\n",
    "    x = Add()([x, conv3_out])\n",
    "    x = convolution_block(x, 128, strides=(2,2,2))\n",
    "    conv4_out = x\n",
    "\n",
    "    # Context 4\n",
    "    x = context_module(x, 128)\n",
    "    x = Add()([x, conv4_out])\n",
    "    x = convolution_block(x, 256, strides=(2,2,2))\n",
    "    \n",
    "    # Context 5\n",
    "    x = context_module(x, 256)\n",
    "\n",
    "    # Global Average Pooling\n",
    "    x = GlobalAveragePooling3D()(x)\n",
    "\n",
    "    # Dropout layer as described in the paper\n",
    "    x = SpatialDropout3D(0.3)(x)   # The paper mentioned a dropout layer after GAP\n",
    "\n",
    "    # Dense layer with 7 output nodes as described in the paper\n",
    "    output = Dense(2, activation='softmax')(x) \n",
    "\n",
    "    model = Model(inputs=input_img, outputs=output)\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'float64'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdicom2nifti\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n",
      "File \u001b[0;32m/export/anaconda/anaconda3/anaconda3-2022.05/lib/python3.9/site-packages/dicom2nifti/__init__.py:19\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mdicom2nifti\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m@author: abrys\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdicom2nifti\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msettings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m disable_validate_slice_increment, \\\n\u001b[1;32m      8\u001b[0m     disable_validate_orientation, \\\n\u001b[1;32m      9\u001b[0m     disable_validate_orthogonal, \\\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     enable_validate_multiframe_implicit, \\\n\u001b[1;32m     18\u001b[0m     enable_resampling\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdicom2nifti\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvert_dicom\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dicom_series_to_nifti\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdicom2nifti\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvert_dir\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert_directory\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdicom2nifti\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpatch_pydicom_encodings\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpatch_pydicom_encodings\u001b[39;00m\n",
      "File \u001b[0;32m/export/anaconda/anaconda3/anaconda3-2022.05/lib/python3.9/site-packages/dicom2nifti/convert_dicom.py:12\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshutil\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtempfile\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnibabel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpydicom_compat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pydicom\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydicom\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tag\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdicom2nifti\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcommon\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/nibabel/__init__.py:40\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124mQuickstart\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124m==========\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124mFor more detailed information see the :ref:`manual`.\u001b[39m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# module imports\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m analyze \u001b[38;5;28;01mas\u001b[39;00m ana\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m spm99analyze \u001b[38;5;28;01mas\u001b[39;00m spm99\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m spm2analyze \u001b[38;5;28;01mas\u001b[39;00m spm2\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/nibabel/analyze.py:87\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m\"\"\" Read / write access to the basic Mayo Analyze format\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m===========================\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03mconstrain the affine.\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvolumeutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (native_code, swapped_code, make_dt_codes,\n\u001b[1;32m     88\u001b[0m                           shape_zoom_affine, array_from_file, seek_tell,\n\u001b[1;32m     89\u001b[0m                           apply_read_scaling)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marraywriters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (make_array_writer, get_slope_inter, WriterError,\n\u001b[1;32m     91\u001b[0m                            ArrayWriter)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrapstruct\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabeledWrapStruct\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/nibabel/volumeutils.py:21\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m reduce\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcasting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m shared_range, OK_FLOATS\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopeners\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Opener, BZ2File, IndexedGzipFile\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecated\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecate_with_version\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/nibabel/casting.py:23\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Test for VC truncation when casting floats to uint64\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Christoph Gohlke says this is so for MSVC <= 2010 because VC is using x87\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# instructions; see:\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# https://github.com/scipy/scipy/blob/99bb8411f6391d921cb3f4e56619291e91ddf43b/scipy/ndimage/tests/test_datatypes.py#L51\u001b[39;00m\n\u001b[1;32m     22\u001b[0m _test_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m63\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m11\u001b[39m  \u001b[38;5;66;03m# Should be exactly representable in float64\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m TRUNC_UINT64 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m(_test_val)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint64) \u001b[38;5;241m!=\u001b[39m _test_val\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfloat_to_int\u001b[39m(arr, int_type, nan2zero\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, infmax\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124;03m\"\"\" Convert floating point array `arr` to type `int_type`\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m    * Rounds numbers to nearest integer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m    standard from that page.\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'float64'"
     ]
    }
   ],
   "source": [
    "import dicom2nifti\n",
    "import glob\n",
    "import math\n",
    "import nibabel as nib\n",
    "import nilearn as nil\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import scipy.ndimage as ndi\n",
    "import statsmodels.stats.contingency_tables as ct\n",
    "import time\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from nilearn import image, plotting\n",
    "from nilearn.input_data import NiftiMasker\n",
    "from nilearn.masking import (apply_mask, compute_brain_mask,\n",
    "                             compute_multi_brain_mask, intersect_masks, unmask)\n",
    "from nilearn.plotting import plot_roi, plot_stat_map, show\n",
    "from numpy import mean, std\n",
    "from numpy.linalg import inv\n",
    "from scipy.stats import chi2_contingency, norm\n",
    "from sklearn import metrics, svm\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import (accuracy_score, confusion_matrix,\n",
    "                             precision_recall_curve, precision_recall_fscore_support,\n",
    "                             roc_auc_score, roc_curve)\n",
    "from sklearn.model_selection import (GridSearchCV, KFold, StratifiedKFold,\n",
    "                                     cross_val_predict, cross_val_score,\n",
    "                                     train_test_split)\n",
    "from sklearn.preprocessing import Binarizer, label_binarize\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate(images,labels,task):\n",
    "    imagesData=[]\n",
    "    labelsData=[]\n",
    "    cn=0\n",
    "    pcn=0\n",
    "    dementia=0\n",
    "    mci=0\n",
    "    for i in range(len(images)):\n",
    "      if labels[i]=='CN':\n",
    "        cn+=1\n",
    "      if labels[i]=='MCI':\n",
    "        mci+=1\n",
    "      if labels[i]=='Dementia':\n",
    "        dementia+=1\n",
    "      if labels[i]=='PCN':\n",
    "        pcn+=1\n",
    "    print(\"Number of CN subjects:\")\n",
    "    print(cn)\n",
    "    print(\"Number of PCN subjects:\")\n",
    "    print(pcn)\n",
    "    print(\"Number of MCI subjects:\")\n",
    "    print(mci)\n",
    "    print(\"Number of Dementia subjects:\")\n",
    "    print(dementia)\n",
    "    if task == \"cd\":\n",
    "        for i in range(len(images)):\n",
    "            if labels[i] == \"CN\":\n",
    "                imagesData.append(images[i])\n",
    "                labelsData.append(labels[i])\n",
    "            if labels[i] == \"Dementia\":\n",
    "                imagesData.append(images[i])\n",
    "                labelsData.append(labels[i])\n",
    "    if task == \"cm\":\n",
    "        for i in range(len(images)):\n",
    "            if labels[i] == \"CN\":\n",
    "                imagesData.append(images[i])\n",
    "                labelsData.append(labels[i])\n",
    "            if labels[i] == \"MCI\":\n",
    "                imagesData.append(images[i])\n",
    "                labelsData.append(labels[i])\n",
    "    if task == \"dm\":\n",
    "        for i in range(len(images)):\n",
    "            if labels[i] == \"Dementia\":\n",
    "                imagesData.append(images[i])\n",
    "                labelsData.append(labels[i])\n",
    "            if labels[i] == \"MCI\":\n",
    "                imagesData.append(images[i])\n",
    "                labelsData.append(labels[i])\n",
    "    if task == \"pc\":\n",
    "        for i in range(len(images)):\n",
    "            if labels[i] == \"PCN\":\n",
    "                imagesData.append(images[i])\n",
    "                labelsData.append(labels[i])\n",
    "            if labels[i] == \"CN\":\n",
    "                imagesData.append(images[i])\n",
    "                labelsData.append(labels[i])   \n",
    "    if task == 'cdm':\n",
    "        for i in range(len(images)):\n",
    "            if labels[i] == \"CN\":\n",
    "                imagesData.append(images[i])\n",
    "                labelsData.append(labels[i])\n",
    "            if labels[i] == \"Dementia\" or labels[i] == 'MCI':\n",
    "                imagesData.append(images[i])\n",
    "                labelsData.append(labels[i])\n",
    "    print(\"lenth of dataset: \")\n",
    "    print(len(labelsData))\n",
    "      \n",
    "        \n",
    "    return imagesData,labelsData\n",
    "\n",
    "\n",
    "def generate_data_path():\n",
    "    files=['/scratch/jjlee/Singularity/ADNI/bids/derivatives/table_preclinical_cross-sectional.csv','/scratch/jjlee/Singularity/ADNI/bids/derivatives/table_cn_cross-sectional.csv','/scratch/jjlee/Singularity/ADNI/bids/derivatives/table_cdr_0p5_apos_cross-sectional.csv','/scratch/jjlee/Singularity/ADNI/bids/derivatives/table_cdr_gt_0p5_apos_cross-sectional.csv']\n",
    "    class_labels=['PCN','CN','MCI','Dementia']\n",
    "    pet_paths = []\n",
    "    mri_paths = []\n",
    "    class_labels_out = []\n",
    "\n",
    "    for file, class_label in zip(files, class_labels):\n",
    "        df = pd.read_csv(file)\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            # Extract sub-xxxx and ses-xxxx from original paths\n",
    "            sub_ses_info = \"/\".join(row['FdgFilename'].split(\"/\")[8:10])\n",
    "\n",
    "            # Generate new directory\n",
    "            new_directory = os.path.join('/scratch/l.peiwang/derivatives_new', sub_ses_info, 'pet')\n",
    "\n",
    "            # Get all files that match the pattern but then exclude ones that contain 'icv'\n",
    "            pet_files = [f for f in glob.glob(new_directory + '/*FDG*') if 'icv' not in f]\n",
    "            mri_files = glob.glob(new_directory + '/*detJ*icv*')\n",
    "            if pet_files and mri_files:  # If both lists are not empty\n",
    "                pet_paths.append(pet_files[0])  # Append the first PET file found\n",
    "                mri_paths.append(mri_files[0])  # Append the first MRI file found\n",
    "                class_labels_out.append(class_label)  # Associate class label with the path\n",
    "\n",
    "    return pet_paths, mri_paths, class_labels_out\n",
    "\n",
    "\n",
    "def binarylabel(train_label,mode):\n",
    "    if mode==\"cd\" or mode==\"cdm\":\n",
    "        for i in range(len(train_label)):\n",
    "            if train_label[i]==\"CN\":\n",
    "                train_label[i]=0\n",
    "            else:\n",
    "                train_label[i]=1\n",
    "    if mode==\"cm\":\n",
    "        for i in range(len(train_label)):\n",
    "            if train_label[i]==\"CN\":\n",
    "                train_label[i]=0\n",
    "            else:\n",
    "                train_label[i]=1\n",
    "    if mode==\"dm\":\n",
    "        for i in range(len(train_label)):\n",
    "            if train_label[i]==\"Dementia\":\n",
    "                train_label[i]=1\n",
    "            else:\n",
    "                train_label[i]=0\n",
    "    if mode==\"pc\":\n",
    "        for i in range(len(train_label)):\n",
    "            if train_label[i]==\"CN\":\n",
    "                train_label[i]=0\n",
    "            else:\n",
    "                train_label[i]=1\n",
    "    if mode==\"pm\":\n",
    "        for i in range(len(train_label)):\n",
    "            if train_label[i]==\"EMCI\":\n",
    "                train_label[i]=1\n",
    "            else:\n",
    "                train_label[i]=0\n",
    "    return train_label\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_data_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m task \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcd\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     35\u001b[0m modality \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPET\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 36\u001b[0m train_data, train_label, masker \u001b[38;5;241m=\u001b[39m \u001b[43mloading_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodality\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mloading_mask\u001b[0;34m(task, modality)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloading_mask\u001b[39m(task,modality):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m#Loading and generating data\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     images_pet,images_mri,labels\u001b[38;5;241m=\u001b[39m\u001b[43mgenerate_data_path\u001b[49m()\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m modality \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPET\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     12\u001b[0m         data_train,train_label\u001b[38;5;241m=\u001b[39mgenerate(images_pet,labels,task)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_data_path' is not defined"
     ]
    }
   ],
   "source": [
    "def augment_data(image):\n",
    "    augmented_image = image.copy()\n",
    "    for axis in range(3):\n",
    "        if np.random.rand() > 0.5:\n",
    "            augmented_image = np.flip(augmented_image, axis=axis)\n",
    "    return augmented_image\n",
    "\n",
    "def loading_mask(task,modality):\n",
    "    #Loading and generating data\n",
    "    images_pet,images_mri,labels=generate_data_path()\n",
    "    if modality == 'PET':\n",
    "        data_train,train_label=generate(images_pet,labels,task)\n",
    "    if modality == 'MRI':\n",
    "        data_train,train_label=generate(images_mri,labels,task)\n",
    "    masker = NiftiMasker(mask_img='/home/l.peiwang/MR-PET-Classfication/mask_gm_p4_new4.nii')\n",
    "    train_data=[]\n",
    "    for i in range(len(data_train)):\n",
    "        # Apply masker and inverse transform\n",
    "        masked_data = masker.fit_transform(data_train[i])\n",
    "        masked_image = masker.inverse_transform(masked_data)\n",
    "\n",
    "        # Resize the image\n",
    "        resized_image = resize(masked_image)\n",
    "\n",
    "        # Convert the resized NIfTI image to a numpy array\n",
    "        resized_data = resized_image.get_fdata()\n",
    "        train_data.append(resized_data)\n",
    "      \n",
    "    train_label=binarylabel(train_label,task)\n",
    "    \n",
    "    return train_data,train_label,masker\n",
    "\n",
    "\n",
    "task = 'cd'\n",
    "modality = 'PET'\n",
    "train_data, train_label, masker = loading_mask(task, modality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(414, 128, 128, 128, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
