{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from nilearn.input_data import NiftiMasker\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "def augment_data(image):\n",
    "    augmented_image = image.copy()\n",
    "    for axis in range(3):\n",
    "        if np.random.rand() > 0.5:\n",
    "            augmented_image = np.flip(augmented_image, axis=axis)\n",
    "    return augmented_image\n",
    "\n",
    "def loading_mask(task,modality):\n",
    "    #Loading and generating data\n",
    "    images_pet,images_mri,labels=generate_data_path()\n",
    "    if modality == 'PET':\n",
    "        data_train,train_label=generate(images_pet,labels,task)\n",
    "    if modality == 'MRI':\n",
    "        data_train,train_label=generate(images_mri,labels,task)\n",
    "    masker = NiftiMasker(mask_img='/home/l.peiwang/MR-PET-Classfication/mask_gm_p4_new4.nii')\n",
    "    train_data=[]\n",
    "    for i in range(len(data_train)):\n",
    "        # Apply masker and inverse transform\n",
    "        masked_data = masker.fit_transform(data_train[i])\n",
    "        masked_image = masker.inverse_transform(masked_data)\n",
    "\n",
    "        # Resize the image\n",
    "        resized_image = resize(masked_image)\n",
    "\n",
    "        # Convert the resized NIfTI image to a numpy array\n",
    "        resized_data = resized_image.get_fdata()\n",
    "        train_data.append(resized_data)\n",
    "      \n",
    "    train_label=binarylabel(train_label,task)\n",
    "    \n",
    "    return train_data,train_label,masker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-29 01:41:25.570049: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-29 01:41:29.042483: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-29 01:41:34.113415: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-29 01:41:34.114457: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy.core'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy.core'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'integer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conv3D, Input, LeakyReLU, Add, GlobalAveragePooling3D, Dense, Dropout, SpatialDropout3D\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregularizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m l2\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/__init__.py:37\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_typing\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyLoader \u001b[38;5;28;01mas\u001b[39;00m _LazyLoader\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/__init__.py:37\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# We aim to keep this file minimal and ideally remove completely.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# If you are adding a new file with @tf_export decorators,\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# import it in modules_with_exports.py instead.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# go/tf-wildcard-import\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Bring in subpackages.\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/context.py:35\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tf_session\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m executor\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m monitoring\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m c_api_util\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m device \u001b[38;5;28;01mas\u001b[39;00m pydev\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/monitoring.py:24\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tfe\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tf_session\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m c_api_util\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_export\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/framework/c_api_util.py:22\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m op_def_pb2\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tf_session \u001b[38;5;28;01mas\u001b[39;00m c_api\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_contextlib\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mAlreadyGarbageCollectedError\u001b[39;00m(\u001b[38;5;167;01mException\u001b[39;00m):\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/util/compat.py:205\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    200\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m as_bytes(path)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# Numpy 1.8 scalars don't inherit from numbers.Integral in Python 3, so we\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;66;03m# need to check them specifically.  The same goes from Real and Complex.\u001b[39;00m\n\u001b[0;32m--> 205\u001b[0m integral_types \u001b[38;5;241m=\u001b[39m (_numbers\u001b[38;5;241m.\u001b[39mIntegral, \u001b[43m_np\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minteger\u001b[49m)\n\u001b[1;32m    206\u001b[0m tf_export(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompat.integral_types\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mexport_constant(\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mintegral_types\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    207\u001b[0m real_types \u001b[38;5;241m=\u001b[39m (_numbers\u001b[38;5;241m.\u001b[39mReal, _np\u001b[38;5;241m.\u001b[39minteger, _np\u001b[38;5;241m.\u001b[39mfloating)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'integer'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Conv3D, Input, LeakyReLU, Add, GlobalAveragePooling3D, Dense, Dropout, SpatialDropout3D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import nibabel as nib\n",
    "\n",
    "import numpy as np\n",
    "from nilearn.image import resample_img, new_img_like, reorder_img\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "def resample_to_spacing(data, original_spacing, new_spacing, interpolation='linear'):\n",
    "    # Assuming the last dimension is the channel and should not be resampled\n",
    "    zoom_factors = [o / n for o, n in zip(original_spacing, new_spacing)] + [1]\n",
    "    return zoom(data, zoom_factors, order=1 if interpolation == 'linear' else 0)\n",
    "\n",
    "\n",
    "def calculate_origin_offset(new_spacing, original_spacing):\n",
    "    return [(o - n) / 2 for o, n in zip(original_spacing, new_spacing)]\n",
    "\n",
    "\n",
    "def pad_image_to_shape(image, target_shape=(128, 128, 128,1)):\n",
    "    # Calculate the padding required in each dimension\n",
    "    padding = [(0, max(target_shape[dim] - image.shape[dim], 0)) for dim in range(3)]\n",
    "    \n",
    "    # Apply zero-padding to the data\n",
    "    new_data = np.pad(image.get_fdata(), padding, mode='constant', constant_values=0)\n",
    "    \n",
    "    # Adjust the affine to account for the new shape\n",
    "    new_affine = np.copy(image.affine)\n",
    "    \n",
    "    # Create and return a new NIfTI-like image with the padded data\n",
    "    return new_img_like(image, new_data, affine=new_affine)\n",
    "\n",
    "\n",
    "\n",
    "def resize(image, new_shape=(128, 128, 128), interpolation=\"linear\"):\n",
    "    # Reorder the image and resample it with the desired interpolation\n",
    "    image = reorder_img(image, resample=interpolation)\n",
    "    \n",
    "    # Calculate the zoom levels needed for the new shape\n",
    "    zoom_level = np.divide(new_shape, image.shape[:3])\n",
    "    \n",
    "    # Calculate the new spacing for the image\n",
    "    new_spacing = np.divide(image.header.get_zooms()[:3], zoom_level)\n",
    "    \n",
    "    # Resample the image data to the new spacing\n",
    "    new_data = resample_to_spacing(image.get_fdata(), image.header.get_zooms()[:3], new_spacing, \n",
    "                                   interpolation=interpolation)\n",
    "    # Copy and adjust the affine transformation matrix for the new spacing\n",
    "    new_affine = np.copy(image.affine)\n",
    "    np.fill_diagonal(new_affine, new_spacing.tolist() + [1])\n",
    "    new_affine[:3, 3] += calculate_origin_offset(new_spacing, image.header.get_zooms()[:3])\n",
    "    \n",
    "    # Create and return a new NIfTI-like image\n",
    "    return new_img_like(image, new_data, affine=new_affine)\n",
    "\n",
    "\n",
    "\n",
    "def convolution_block(x, filters, kernel_size=(3,3,3), strides=(1,1,1)):\n",
    "    x = Conv3D(filters, kernel_size, strides=strides, padding='same', kernel_regularizer=l2(1e-5))(x)\n",
    "    x = tfa.layers.InstanceNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    return x\n",
    "\n",
    "def context_module(x, filters):\n",
    "    # First convolution block\n",
    "    x = convolution_block(x, filters)\n",
    "    # Dropout layer\n",
    "    x = SpatialDropout3D(0.3)(x) \n",
    "    # Second convolution block\n",
    "    x = convolution_block(x, filters)\n",
    "    return x\n",
    "\n",
    "def create_cnn_model():\n",
    "    input_img = Input(shape=(128, 128, 128, 1))\n",
    "    x = convolution_block(input_img, 16, strides=(1,1,1))\n",
    "    conv1_out = x\n",
    "\n",
    "    # Context 1\n",
    "    x = context_module(x, 16)\n",
    "    x = Add()([x, conv1_out])\n",
    "    x = convolution_block(x, 32, strides=(2,2,2))\n",
    "    conv2_out = x\n",
    "\n",
    "    # Context 2\n",
    "    x = context_module(x, 32)\n",
    "    x = Add()([x, conv2_out])\n",
    "    x = convolution_block(x, 64, strides=(2,2,2))\n",
    "    conv3_out = x\n",
    "\n",
    "    # Context 3\n",
    "    x = context_module(x, 64)\n",
    "    x = Add()([x, conv3_out])\n",
    "    x = convolution_block(x, 128, strides=(2,2,2))\n",
    "    conv4_out = x\n",
    "\n",
    "    # Context 4\n",
    "    x = context_module(x, 128)\n",
    "    x = Add()([x, conv4_out])\n",
    "    x = convolution_block(x, 256, strides=(2,2,2))\n",
    "    \n",
    "    # Context 5\n",
    "    x = context_module(x, 256)\n",
    "\n",
    "    # Global Average Pooling\n",
    "    x = GlobalAveragePooling3D()(x)\n",
    "\n",
    "    # Dropout layer as described in the paper\n",
    "    x = SpatialDropout3D(0.3)(x)   # The paper mentioned a dropout layer after GAP\n",
    "\n",
    "    # Dense layer with 7 output nodes as described in the paper\n",
    "    output = Dense(2, activation='softmax')(x) \n",
    "\n",
    "    model = Model(inputs=input_img, outputs=output)\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'float64'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdicom2nifti\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n",
      "File \u001b[0;32m/export/anaconda/anaconda3/anaconda3-2022.05/lib/python3.9/site-packages/dicom2nifti/__init__.py:19\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mdicom2nifti\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m@author: abrys\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdicom2nifti\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msettings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m disable_validate_slice_increment, \\\n\u001b[1;32m      8\u001b[0m     disable_validate_orientation, \\\n\u001b[1;32m      9\u001b[0m     disable_validate_orthogonal, \\\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     enable_validate_multiframe_implicit, \\\n\u001b[1;32m     18\u001b[0m     enable_resampling\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdicom2nifti\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvert_dicom\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dicom_series_to_nifti\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdicom2nifti\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvert_dir\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert_directory\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdicom2nifti\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpatch_pydicom_encodings\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpatch_pydicom_encodings\u001b[39;00m\n",
      "File \u001b[0;32m/export/anaconda/anaconda3/anaconda3-2022.05/lib/python3.9/site-packages/dicom2nifti/convert_dicom.py:12\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshutil\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtempfile\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnibabel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpydicom_compat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pydicom\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydicom\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tag\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdicom2nifti\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcommon\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/nibabel/__init__.py:40\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124mQuickstart\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124m==========\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124mFor more detailed information see the :ref:`manual`.\u001b[39m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# module imports\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m analyze \u001b[38;5;28;01mas\u001b[39;00m ana\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m spm99analyze \u001b[38;5;28;01mas\u001b[39;00m spm99\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m spm2analyze \u001b[38;5;28;01mas\u001b[39;00m spm2\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/nibabel/analyze.py:87\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m\"\"\" Read / write access to the basic Mayo Analyze format\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m===========================\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03mconstrain the affine.\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvolumeutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (native_code, swapped_code, make_dt_codes,\n\u001b[1;32m     88\u001b[0m                           shape_zoom_affine, array_from_file, seek_tell,\n\u001b[1;32m     89\u001b[0m                           apply_read_scaling)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marraywriters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (make_array_writer, get_slope_inter, WriterError,\n\u001b[1;32m     91\u001b[0m                            ArrayWriter)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrapstruct\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabeledWrapStruct\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/nibabel/volumeutils.py:21\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m reduce\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcasting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m shared_range, OK_FLOATS\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopeners\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Opener, BZ2File, IndexedGzipFile\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecated\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecate_with_version\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/nibabel/casting.py:23\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Test for VC truncation when casting floats to uint64\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Christoph Gohlke says this is so for MSVC <= 2010 because VC is using x87\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# instructions; see:\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# https://github.com/scipy/scipy/blob/99bb8411f6391d921cb3f4e56619291e91ddf43b/scipy/ndimage/tests/test_datatypes.py#L51\u001b[39;00m\n\u001b[1;32m     22\u001b[0m _test_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m63\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m11\u001b[39m  \u001b[38;5;66;03m# Should be exactly representable in float64\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m TRUNC_UINT64 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m(_test_val)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint64) \u001b[38;5;241m!=\u001b[39m _test_val\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfloat_to_int\u001b[39m(arr, int_type, nan2zero\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, infmax\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124;03m\"\"\" Convert floating point array `arr` to type `int_type`\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m    * Rounds numbers to nearest integer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m    standard from that page.\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'float64'"
     ]
    }
   ],
   "source": [
    "import dicom2nifti\n",
    "import glob\n",
    "import math\n",
    "import nibabel as nib\n",
    "import nilearn as nil\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import scipy.ndimage as ndi\n",
    "import statsmodels.stats.contingency_tables as ct\n",
    "import time\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from nilearn import image, plotting\n",
    "from nilearn.input_data import NiftiMasker\n",
    "from nilearn.masking import (apply_mask, compute_brain_mask,\n",
    "                             compute_multi_brain_mask, intersect_masks, unmask)\n",
    "from nilearn.plotting import plot_roi, plot_stat_map, show\n",
    "from numpy import mean, std\n",
    "from numpy.linalg import inv\n",
    "from scipy.stats import chi2_contingency, norm\n",
    "from sklearn import metrics, svm\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import (accuracy_score, confusion_matrix,\n",
    "                             precision_recall_curve, precision_recall_fscore_support,\n",
    "                             roc_auc_score, roc_curve)\n",
    "from sklearn.model_selection import (GridSearchCV, KFold, StratifiedKFold,\n",
    "                                     cross_val_predict, cross_val_score,\n",
    "                                     train_test_split)\n",
    "from sklearn.preprocessing import Binarizer, label_binarize\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate(images,labels,task):\n",
    "    imagesData=[]\n",
    "    labelsData=[]\n",
    "    cn=0\n",
    "    pcn=0\n",
    "    dementia=0\n",
    "    mci=0\n",
    "    for i in range(len(images)):\n",
    "      if labels[i]=='CN':\n",
    "        cn+=1\n",
    "      if labels[i]=='MCI':\n",
    "        mci+=1\n",
    "      if labels[i]=='Dementia':\n",
    "        dementia+=1\n",
    "      if labels[i]=='PCN':\n",
    "        pcn+=1\n",
    "    print(\"Number of CN subjects:\")\n",
    "    print(cn)\n",
    "    print(\"Number of PCN subjects:\")\n",
    "    print(pcn)\n",
    "    print(\"Number of MCI subjects:\")\n",
    "    print(mci)\n",
    "    print(\"Number of Dementia subjects:\")\n",
    "    print(dementia)\n",
    "    if task == \"cd\":\n",
    "        for i in range(len(images)):\n",
    "            if labels[i] == \"CN\":\n",
    "                imagesData.append(images[i])\n",
    "                labelsData.append(labels[i])\n",
    "            if labels[i] == \"Dementia\":\n",
    "                imagesData.append(images[i])\n",
    "                labelsData.append(labels[i])\n",
    "    if task == \"cm\":\n",
    "        for i in range(len(images)):\n",
    "            if labels[i] == \"CN\":\n",
    "                imagesData.append(images[i])\n",
    "                labelsData.append(labels[i])\n",
    "            if labels[i] == \"MCI\":\n",
    "                imagesData.append(images[i])\n",
    "                labelsData.append(labels[i])\n",
    "    if task == \"dm\":\n",
    "        for i in range(len(images)):\n",
    "            if labels[i] == \"Dementia\":\n",
    "                imagesData.append(images[i])\n",
    "                labelsData.append(labels[i])\n",
    "            if labels[i] == \"MCI\":\n",
    "                imagesData.append(images[i])\n",
    "                labelsData.append(labels[i])\n",
    "    if task == \"pc\":\n",
    "        for i in range(len(images)):\n",
    "            if labels[i] == \"PCN\":\n",
    "                imagesData.append(images[i])\n",
    "                labelsData.append(labels[i])\n",
    "            if labels[i] == \"CN\":\n",
    "                imagesData.append(images[i])\n",
    "                labelsData.append(labels[i])   \n",
    "    if task == 'cdm':\n",
    "        for i in range(len(images)):\n",
    "            if labels[i] == \"CN\":\n",
    "                imagesData.append(images[i])\n",
    "                labelsData.append(labels[i])\n",
    "            if labels[i] == \"Dementia\" or labels[i] == 'MCI':\n",
    "                imagesData.append(images[i])\n",
    "                labelsData.append(labels[i])\n",
    "    print(\"lenth of dataset: \")\n",
    "    print(len(labelsData))\n",
    "      \n",
    "        \n",
    "    return imagesData,labelsData\n",
    "\n",
    "\n",
    "def generate_data_path():\n",
    "    files=['/scratch/jjlee/Singularity/ADNI/bids/derivatives/table_preclinical_cross-sectional.csv','/scratch/jjlee/Singularity/ADNI/bids/derivatives/table_cn_cross-sectional.csv','/scratch/jjlee/Singularity/ADNI/bids/derivatives/table_cdr_0p5_apos_cross-sectional.csv','/scratch/jjlee/Singularity/ADNI/bids/derivatives/table_cdr_gt_0p5_apos_cross-sectional.csv']\n",
    "    class_labels=['PCN','CN','MCI','Dementia']\n",
    "    pet_paths = []\n",
    "    mri_paths = []\n",
    "    class_labels_out = []\n",
    "\n",
    "    for file, class_label in zip(files, class_labels):\n",
    "        df = pd.read_csv(file)\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            # Extract sub-xxxx and ses-xxxx from original paths\n",
    "            sub_ses_info = \"/\".join(row['FdgFilename'].split(\"/\")[8:10])\n",
    "\n",
    "            # Generate new directory\n",
    "            new_directory = os.path.join('/scratch/l.peiwang/derivatives_new', sub_ses_info, 'pet')\n",
    "\n",
    "            # Get all files that match the pattern but then exclude ones that contain 'icv'\n",
    "            pet_files = [f for f in glob.glob(new_directory + '/*FDG*') if 'icv' not in f]\n",
    "            mri_files = glob.glob(new_directory + '/*detJ*icv*')\n",
    "            if pet_files and mri_files:  # If both lists are not empty\n",
    "                pet_paths.append(pet_files[0])  # Append the first PET file found\n",
    "                mri_paths.append(mri_files[0])  # Append the first MRI file found\n",
    "                class_labels_out.append(class_label)  # Associate class label with the path\n",
    "\n",
    "    return pet_paths, mri_paths, class_labels_out\n",
    "\n",
    "\n",
    "def binarylabel(train_label,mode):\n",
    "    if mode==\"cd\" or mode==\"cdm\":\n",
    "        for i in range(len(train_label)):\n",
    "            if train_label[i]==\"CN\":\n",
    "                train_label[i]=0\n",
    "            else:\n",
    "                train_label[i]=1\n",
    "    if mode==\"cm\":\n",
    "        for i in range(len(train_label)):\n",
    "            if train_label[i]==\"CN\":\n",
    "                train_label[i]=0\n",
    "            else:\n",
    "                train_label[i]=1\n",
    "    if mode==\"dm\":\n",
    "        for i in range(len(train_label)):\n",
    "            if train_label[i]==\"Dementia\":\n",
    "                train_label[i]=1\n",
    "            else:\n",
    "                train_label[i]=0\n",
    "    if mode==\"pc\":\n",
    "        for i in range(len(train_label)):\n",
    "            if train_label[i]==\"CN\":\n",
    "                train_label[i]=0\n",
    "            else:\n",
    "                train_label[i]=1\n",
    "    if mode==\"pm\":\n",
    "        for i in range(len(train_label)):\n",
    "            if train_label[i]==\"EMCI\":\n",
    "                train_label[i]=1\n",
    "            else:\n",
    "                train_label[i]=0\n",
    "    return train_label\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_data_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m task \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcd\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     35\u001b[0m modality \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPET\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 36\u001b[0m train_data, train_label, masker \u001b[38;5;241m=\u001b[39m \u001b[43mloading_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodality\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mloading_mask\u001b[0;34m(task, modality)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloading_mask\u001b[39m(task,modality):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m#Loading and generating data\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     images_pet,images_mri,labels\u001b[38;5;241m=\u001b[39m\u001b[43mgenerate_data_path\u001b[49m()\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m modality \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPET\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     12\u001b[0m         data_train,train_label\u001b[38;5;241m=\u001b[39mgenerate(images_pet,labels,task)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_data_path' is not defined"
     ]
    }
   ],
   "source": [
    "def augment_data(image):\n",
    "    augmented_image = image.copy()\n",
    "    for axis in range(3):\n",
    "        if np.random.rand() > 0.5:\n",
    "            augmented_image = np.flip(augmented_image, axis=axis)\n",
    "    return augmented_image\n",
    "\n",
    "def loading_mask(task,modality):\n",
    "    #Loading and generating data\n",
    "    images_pet,images_mri,labels=generate_data_path()\n",
    "    if modality == 'PET':\n",
    "        data_train,train_label=generate(images_pet,labels,task)\n",
    "    if modality == 'MRI':\n",
    "        data_train,train_label=generate(images_mri,labels,task)\n",
    "    masker = NiftiMasker(mask_img='/home/l.peiwang/MR-PET-Classfication/mask_gm_p4_new4.nii')\n",
    "    train_data=[]\n",
    "    for i in range(len(data_train)):\n",
    "        # Apply masker and inverse transform\n",
    "        masked_data = masker.fit_transform(data_train[i])\n",
    "        masked_image = masker.inverse_transform(masked_data)\n",
    "\n",
    "        # Resize the image\n",
    "        resized_image = resize(masked_image)\n",
    "\n",
    "        # Convert the resized NIfTI image to a numpy array\n",
    "        resized_data = resized_image.get_fdata()\n",
    "        train_data.append(resized_data)\n",
    "      \n",
    "    train_label=binarylabel(train_label,task)\n",
    "    \n",
    "    return train_data,train_label,masker\n",
    "\n",
    "\n",
    "task = 'cd'\n",
    "modality = 'PET'\n",
    "train_data, train_label, masker = loading_mask(task, modality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(414, 128, 128, 128, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
